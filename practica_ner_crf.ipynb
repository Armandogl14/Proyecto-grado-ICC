{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import spacy\n",
        "import string\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Cargar el modelo de spaCy para características lingüísticas\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word2features(sent, i):\n",
        "    \"\"\"\n",
        "    Extrae características avanzadas para cada palabra en la secuencia\n",
        "    \"\"\"\n",
        "    word = sent[i][0]\n",
        "    \n",
        "    # Análisis con spaCy\n",
        "    doc = nlp(word)\n",
        "    token = doc[0]\n",
        "    \n",
        "    # Características básicas\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'word.has_hyphen': '-' in word,\n",
        "        'word.has_digit': any(char.isdigit() for char in word),\n",
        "        'word.has_punct': any(char in string.punctuation for char in word),\n",
        "        \n",
        "        # Características lingüísticas de spaCy\n",
        "        'pos': token.pos_,\n",
        "        'dep': token.dep_,\n",
        "        'is_stop': token.is_stop,\n",
        "        'is_alpha': token.is_alpha,\n",
        "        'is_punct': token.is_punct,\n",
        "        'like_num': token.like_num,\n",
        "        'shape': token.shape_,\n",
        "    }\n",
        "    \n",
        "    # Características de la palabra anterior\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:word.has_digit': any(char.isdigit() for char in word1),\n",
        "            '-1:word.has_punct': any(char in string.punctuation for char in word1),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    \n",
        "    # Características de la palabra siguiente\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:word.has_digit': any(char.isdigit() for char in word1),\n",
        "            '+1:word.has_punct': any(char in string.punctuation for char in word1),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "        \n",
        "    # Características de ventana\n",
        "    if i > 1:\n",
        "        word2 = sent[i-2][0]\n",
        "        features['-2:word.lower()'] = word2.lower()\n",
        "    if i < len(sent)-2:\n",
        "        word2 = sent[i+2][0]\n",
        "        features['+2:word.lower()'] = word2.lower()\n",
        "    \n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "\n",
        "# Convertir los datos al formato para CRF\n",
        "X_train = [sent2features(s) for s in train_data]\n",
        "y_train = [sent2labels(s) for s in train_data]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_data]\n",
        "y_test = [sent2labels(s) for s in test_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir el espacio de hiperparámetros para la búsqueda\n",
        "params_space = {\n",
        "    'c1': np.logspace(-3, 3, 7),  # Coeficiente de regularización L1\n",
        "    'c2': np.logspace(-3, 3, 7),  # Coeficiente de regularización L2\n",
        "    'max_iterations': [50, 100, 150],\n",
        "    'feature.possible_transitions': [True, False],\n",
        "    'feature.possible_states': [True, False]\n",
        "}\n",
        "\n",
        "# Crear el modelo CRF base\n",
        "crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "# Realizar búsqueda aleatoria de hiperparámetros con validación cruzada\n",
        "rs = RandomizedSearchCV(\n",
        "    estimator=crf,\n",
        "    param_distributions=params_space,\n",
        "    n_iter=20,  # Número de combinaciones a probar\n",
        "    cv=3,       # Número de folds para validación cruzada\n",
        "    verbose=1,\n",
        "    n_jobs=-1   # Usar todos los núcleos disponibles\n",
        ")\n",
        "\n",
        "# Entrenar el modelo con búsqueda de hiperparámetros\n",
        "print(\"Iniciando búsqueda de hiperparámetros...\")\n",
        "rs.fit(X_train, y_train)\n",
        "\n",
        "# Obtener y mostrar los mejores hiperparámetros\n",
        "print(\"\\nMejores hiperparámetros encontrados:\")\n",
        "for param, value in rs.best_params_.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "# Entrenar el modelo final con los mejores hiperparámetros\n",
        "best_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    **rs.best_params_\n",
        ")\n",
        "\n",
        "print(\"\\nEntrenando modelo final con los mejores hiperparámetros...\")\n",
        "best_crf.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar el modelo en el conjunto de prueba\n",
        "y_pred = best_crf.predict(X_test)\n",
        "\n",
        "# Generar reporte de clasificación detallado\n",
        "print(\"Reporte de clasificación detallado:\")\n",
        "print(flat_classification_report(y_test, y_pred))\n",
        "\n",
        "# Análisis de errores\n",
        "def analyze_errors(y_true, y_pred, X_test, test_data):\n",
        "    errors = []\n",
        "    for i, (true_seq, pred_seq, features_seq, original_sent) in enumerate(zip(y_true, y_pred, X_test, test_data)):\n",
        "        for j, (true_label, pred_label) in enumerate(zip(true_seq, pred_seq)):\n",
        "            if true_label != pred_label:\n",
        "                word = original_sent[j][0]\n",
        "                context = ' '.join([token[0] for token in original_sent[max(0,j-2):min(len(original_sent),j+3)]])\n",
        "                errors.append({\n",
        "                    'word': word,\n",
        "                    'true_label': true_label,\n",
        "                    'pred_label': pred_label,\n",
        "                    'context': context,\n",
        "                    'features': features_seq[j]\n",
        "                })\n",
        "    return errors\n",
        "\n",
        "print(\"\\nAnálisis de errores:\")\n",
        "errors = analyze_errors(y_test, y_pred, X_test, test_data)\n",
        "error_df = pd.DataFrame(errors)\n",
        "\n",
        "print(\"\\nDistribución de errores por tipo de entidad:\")\n",
        "error_counts = error_df.groupby(['true_label', 'pred_label']).size().unstack(fill_value=0)\n",
        "print(error_counts)\n",
        "\n",
        "# Guardar los errores en un archivo CSV para análisis posterior\n",
        "error_df.to_csv('error_analysis.csv', index=False)\n",
        "print(\"\\nAnálisis de errores guardado en 'error_analysis.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para predecir entidades en texto nuevo\n",
        "def predict_entities(text, crf_model):\n",
        "    # Tokenizar el texto\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Crear una secuencia de características\n",
        "    sent = [(token, 'O') for token in tokens]  # 'O' es un placeholder\n",
        "    X = sent2features(sent)\n",
        "    \n",
        "    # Predecir las etiquetas\n",
        "    y_pred = crf_model.predict([X])[0]\n",
        "    \n",
        "    # Extraer las entidades\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    \n",
        "    for i, (token, label) in enumerate(zip(tokens, y_pred)):\n",
        "        if label.startswith('B-'):\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            current_entity = {'text': token, 'type': label[2:], 'start': i}\n",
        "        elif label.startswith('I-') and current_entity and label[2:] == current_entity['type']:\n",
        "            current_entity['text'] += ' ' + token\n",
        "        else:\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            current_entity = None\n",
        "    \n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    return entities\n",
        "\n",
        "# Probar el modelo con algunas oraciones de ejemplo\n",
        "test_sentences = [\n",
        "    \"The Thai restaurant on Main Street has great pad thai for $12.99\",\n",
        "    \"I had dinner at Le Bernardin last night and the service was excellent\",\n",
        "    \"The prices at Sushi Express are very reasonable, especially their lunch special\"\n",
        "]\n",
        "\n",
        "print(\"Ejemplos de predicciones del modelo CRF:\")\n",
        "for sentence in test_sentences:\n",
        "    print(f\"\\nTexto: {sentence}\")\n",
        "    entities = predict_entities(sentence, best_crf)\n",
        "    print(\"Entidades encontradas:\")\n",
        "    for entity in entities:\n",
        "        print(f\"- {entity['text']}: {entity['type']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
